# 00\_ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨è¨“ç·´

## å­¦ã¶ã¹ãé …ç›®ã¯ä»¥ä¸‹

- TensorFlow2.0 ä»¥é™ã‚’ä½¿ç”¨ã™ã‚‹ã€‚

- [tutorial1](#tutorial1)

  - [x] TensorFlow ã‚’ä½¿ç”¨ã—ã¦æ©Ÿæ¢°å­¦ç¿’(ML)ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã€è¨“ç·´ã‚’è¡Œã†ã€‚
  - [ ] ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã—ã¦ãƒ¢ãƒ‡ãƒ«ã§ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
  - [x] ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦çµæœã‚’äºˆæ¸¬ã™ã‚‹ã€‚
  - [x] è¤‡æ•°ã®å±¤ã§æ§‹æˆã•ã‚Œã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
  - [x] äºŒé …åˆ†é¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¦è¨“ç·´ã™ã‚‹ã€‚
  - [x] å¤šé …åˆ†é¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¦è¨“ç·´ã™ã‚‹ã€‚

- [tutorial2](#tutorial2)

  - [ ] äº‹å‰è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹(è»¢ç§»å­¦ç¿’)ã€‚
  - [ ] äº‹å‰è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ©Ÿèƒ½ã‚’æŠ½å‡ºã™ã‚‹ã€‚
  - [ ] ãƒ¢ãƒ‡ãƒ«ã¸ã®å…¥åŠ›ãŒé©åˆ‡ãªå½¢çŠ¶ã§è¡Œã‚ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
  - [ ] ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¥åŠ›ã®å½¢çŠ¶ã«åˆã‚ã›ãŸã‚‚ã®ã«ã™ã‚‹ã€‚
  - [ ] ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ« ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§æŒ‡å®šã•ã‚ŒãŸå…¥åŠ›ã®å½¢çŠ¶ã«åˆã‚ã›ãŸã‚‚ã®ã«ã™ã‚‹ã€‚

- [tutorial3](#tutorial3)

  - [x] ãƒ‡ãƒ¼ã‚¿ã®ä¸€æ‹¬èª­ã¿è¾¼ã¿ã«ã¤ã„ã¦ç†è§£ã—ã¦ã„ã‚‹
  - [x] ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦ã€è¨“ç·´ã‚µã‚¤ã‚¯ãƒ«ã®çµ‚äº†ã‚’å‘¼ã³å‡ºã™ã€‚
  - [ ] è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã€‚
  - [ ] è¤‡æ•°ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ(json ã‚„ csv ãªã©)ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã€‚
  - [ ] tf.data.datasets ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã€‚

- [éå­¦ç¿’](#OverFitting)

  - [x] è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ­ãƒƒãƒˆã®æå¤±ã¨ç²¾åº¦ã‚’ç¢ºèªã™ã‚‹ã€‚
  - [x] æ‹¡å¼µã‚„ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆãªã©ã®éå‰°é©åˆã‚’é¿ã‘ã‚‹ãŸã‚ã®æˆ¦ç•¥ã‚’å‰²ã‚Šå‡ºã™ã€‚

---

## <a name="tutorial1">tutorial1</a>

- [x] TensorFlow ã‚’ä½¿ç”¨ã—ã¦æ©Ÿæ¢°å­¦ç¿’(ML)ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã€è¨“ç·´ã‚’è¡Œã†ã€‚
- [x] ãƒ‡ãƒ¼ã‚¿ã‚’å‰å‡¦ç†ã—ã¦ãƒ¢ãƒ‡ãƒ«ã§ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
- [x] ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦çµæœã‚’äºˆæ¸¬ã™ã‚‹ã€‚

```python
"""ğŸŒŸ ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰
"""
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

"""ğŸŒŸ ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
"""
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

"""ğŸŒŸ ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
"""
model.fit(train_images, train_labels, epochs=5)

"""ğŸŒŸ ãƒ¢ãƒ‡ãƒ«ã®æ­£è§£ç‡ã®è©•ä¾¡
"""
test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)
print('Test accuracy:', test_acc)

"""
    ğŸŒŸ äºˆæ¸¬
        - å…¥åŠ›ã«ã¯å…¥åŠ›ã®é…åˆ—ã‚’å…¥ã‚Œã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
        - å¤šé …åˆ†é¡ã®å ´åˆã€10å€‹ã®é…åˆ—ã§æ¸¡ã•ã‚Œã‚‹ã€‚
          10å€‹ã®ä¸­ã‹ã‚‰æœ€å¤§å€¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒ‡å®šã—ã¦äºˆæ¸¬çµæœã¨ã™ã‚‹ã€‚
        - äºŒé …åˆ†é¡ã®å ´åˆã€1ã¤ã®æ•°å­—(ç¢ºç‡)ã§æ¸¡ã•ã‚Œã‚‹ã€‚
"""
predictions = model.predict(test_images)
np.argmax(predictions[0])

```

- [x] è¤‡æ•°ã®å±¤ã§æ§‹æˆã•ã‚Œã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
- [x] äºŒé …åˆ†é¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¦è¨“ç·´ã™ã‚‹ã€‚
- [x] å¤šé …åˆ†é¡ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¦è¨“ç·´ã™ã‚‹ã€‚

### äºŒé …åˆ†é¡

```python
# å…¥åŠ›ã®å½¢å¼ã¯æ˜ ç”»ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹èªå½™æ•°ï¼ˆ10,000èªï¼‰
vocab_size = 10000

"""
    ğŸŒŸ äºŒé …ã—ã‹ãªã„ãŸã‚ã€0,1ã‚’ã©ã¡ã‚‰ã‹ã«å‰²ã‚ŠæŒ¯ã£ã¦
    ğŸŒŸ softmaxã§0~1ã®å€¤ã¨ãªã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚(ãŠãã‚‰ãå¤šé …åˆ†é¡ã®äºŒé …ã«ã—ã¦ã‚‚ã„ã„ã‚“ã˜ã‚ƒãªã„ã‹ãªï¼Ÿ)
"""

model = keras.Sequential()
model.add(keras.layers.Embedding(vocab_size, 16))
model.add(keras.layers.GlobalAveragePooling1D())
model.add(keras.layers.Dense(16, activation='relu'))
model.add(keras.layers.Dense(1, activation='sigmoid'))

"""
    ğŸŒŸ äºŒå€¤åˆ†é¡å•é¡Œã§ã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã¯ç¢ºç‡ï¼ˆ1ãƒ¦ãƒ‹ãƒƒãƒˆã®å±¤ã¨ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰æ´»æ€§åŒ–é–¢æ•°ï¼‰ã®å ´åˆã€
    ğŸŒŸ binary_crossentropyã‚’ä½¿ãˆã‚‹ã€‚
    (å›å¸°å•é¡Œ(å®¶å±‹ã®å€¤æ®µã‚’æ¨å®šã™ã‚‹ã¨ã‹)ã®å ´åˆã€mean_squared_errorï¼ˆå¹³å‡äºŒä¹—èª¤å·®ï¼‰ã‚’ä½¿ã†ã“ã¨ã‚‚ã§ãã‚‹ã€‚)
"""

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
```

### å¤šé …åˆ†é¡

- ä»¥ä¸‹ã®ã‚ˆã†ã«ã€å‡ºåŠ›å±¤ã‚’å¤šé …æ•°ã ã‘ç”¨æ„ã—ã€loss é–¢æ•°ã«é©åˆ‡ãªé–¢æ•°ã‚’æŒ‡å®šã™ã‚Œã° OK

```python
"""
    ğŸŒŸ å¤šé …åˆ†é¡ã®ãŸã‚ã€å‡ºåŠ›å±¤ã‚’æ±‚ã‚ã‚‹ã‚¯ãƒ©ã‚¹ã®æ•°ã ã‘å‡ºåŠ›ã•ã›ã€
    ğŸŒŸ softmaxã§0~1ã®å€¤ã¨ãªã‚‹ã‚ˆã†ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
"""
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])

"""
    ğŸŒŸ å¤šé …åˆ†é¡ã«ãŠã„ã¦lossé–¢æ•°ã®è©•ä¾¡ã¯å¤šé …åˆ†ã®è©•ä¾¡ã«ã‚ã£ãŸã‚‚ã®ã‚’é¸ã¶å¿…è¦ãŒã‚ã‚‹ã€‚
    ä»Šå›ã®å ´åˆã€ä»¥ä¸‹ãªã©ãŒé¸ã°ã‚Œã‚‹ã€‚
        sparse_categorical_crossentropy
"""

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
```

### å›å¸°å•é¡Œ

```python
def build_model():
  model = keras.Sequential([
    layers.Dense(64, activation='relu',
        # tuple or listå½¢å¼ã§æ¸¡ã™å¿…è¦ãŒã‚ã‚‹ã€‚(æ•°å­—)ã ã¨tupleã¨èªè­˜ã—ãªã„ã®ã§(æ•°å­—,)ã¨ã™ã‚‹ã“ã¨ã§1Dã§æ¸¡ã™ã“ã¨ãŒã§ãã‚‹ã€‚
        input_shape=[len(train_dataset.keys())]),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)
  ])

  optimizer = tf.keras.optimizers.RMSprop(0.001)

  """
    ğŸŒŸ lossé–¢æ•°ã«mse(mean_squared_errorï¼ˆå¹³å‡äºŒä¹—èª¤å·®))ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€è©•ä¾¡ã—ã¦ã„ã‚‹ã€‚
    ğŸŒŸ metricã«ã¯mse,maeã¨ã‹ã‚’é¸æŠã™ã‚‹ã€‚
  """
  model.compile(loss='mse',
                optimizer=optimizer,
                metrics=['mae', 'mse'])
  return model
```

---

## <a name="tutorial2">tutorial2</a>

- [ ] äº‹å‰è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹(è»¢ç§»å­¦ç¿’)ã€‚
- [ ] äº‹å‰è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ©Ÿèƒ½ã‚’æŠ½å‡ºã™ã‚‹ã€‚
- [ ] ãƒ¢ãƒ‡ãƒ«ã¸ã®å…¥åŠ›ãŒé©åˆ‡ãªå½¢çŠ¶ã§è¡Œã‚ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
- [ ] ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¥åŠ›ã®å½¢çŠ¶ã«åˆã‚ã›ãŸã‚‚ã®ã«ã™ã‚‹ã€‚
- [ ] ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ« ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§æŒ‡å®šã•ã‚ŒãŸå…¥åŠ›ã®å½¢çŠ¶ã«åˆã‚ã›ãŸã‚‚ã®ã«ã™ã‚‹ã€‚

```python

```

---

## <a name="tutorial3">tutorial3</a>

- [x] [ãƒ‡ãƒ¼ã‚¿ã®ä¸€æ‹¬èª­ã¿è¾¼ã¿ã«ã¤ã„ã¦ç†è§£ã—ã¦ã„ã‚‹](#LoadData)
- [x] [ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦ã€è¨“ç·´ã‚µã‚¤ã‚¯ãƒ«ã®çµ‚äº†ã‚’å‘¼ã³å‡ºã™ã€‚](#Callback)
- [ ] è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã€‚
- [ ] è¤‡æ•°ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ(json ã‚„ csv ãªã©)ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã€‚
- [ ] tf.data.datasets ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã€‚

```python

```

### <a name=LoadData>ãƒ‡ãƒ¼ã‚¿ã®ä¸€æ‹¬èª­ã¿è¾¼ã¿ã«ã¤ã„ã¦ç†è§£ã—ã¦ã„ã‚‹</a>

- ç”»åƒã®èª­ã¿è¾¼ã¿
  - ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®éšå±¤ã§ãƒ©ãƒ™ãƒ«ã‚’åˆ†ã‘ã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’å‰æã«è¨˜è¿°ã™ã‚‹ã€‚

```python
import tensorflow as tf
AUTOTUNE = tf.data.experimental.AUTOTUNE # ğŸŒŸ åˆ¥ã«ãªãã¦ã‚‚ã„ã„ã€å¤‰æ•°ã¨ã—ã¦ç¢ºä¿ã—ã¦ãŠã„ã¦ã„ã‚‹ã ã‘
```

```python
import pathlib

# ğŸŒŸ flower dataset ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
data_root_orig =
    tf.keras.utils.get_file(origin='https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
    fname='flower_photos',
    untar=True
)
data_root = pathlib.Path(data_root_orig)
print(data_root)

# ğŸŒŸ all_image_pathsã«ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»åƒã®ãƒ‘ã‚¹ãŒæ ¼ç´ã•ã‚Œã‚‹ã€‚
all_image_paths = list(data_root.glob('*/*'))
all_image_paths = [str(path) for path in all_image_paths]

import random
random.shuffle(all_image_paths)

# ğŸŒŸç”»åƒãƒ‘ã‚¹ã®ãƒ‡ã‚³ãƒ¼ãƒ‰æ–¹æ³•
img_raw = tf.io.read_file(img_path) # ğŸŒŸã“ã®æ®µéšã§ã¯ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒ†ãƒ³ã‚½ãƒ«
img_tensor = tf.image.decode_image(img_raw) # ğŸŒŸ ã“ã“ã§3chã®ç”»åƒã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã•ã‚Œã‚‹
img_final = tf.image.resize(img_tensor, [192, 192]) # ãƒªã‚µã‚¤ã‚º
img_final = img_final/255.0 # rescale

# --------------------------------------- #
# ğŸŒŸé–¢æ•°ã«ã¾ã¨ã‚ã‚‹ã¨ã“ã‚“ãªæ„Ÿã˜ğŸŒŸ
def preprocess_image(image):
  image = tf.image.decode_jpeg(image, channels=3)
  image = tf.image.resize(image, [192, 192])
  image /= 255.0  # normalize to [0,1] range

  return image

def load_and_preprocess_image(path):
  image = tf.io.read_file(path)
  return preprocess_image(image)
# --------------------------------------- #


# ğŸŒŸã€€ä»¥ä¸‹ã®ã‚ˆã†ã«ã—ã¦ä½¿ã†ã“ã¨ã§ç”»åƒã‚’å®Ÿè¡Œæ™‚ã«ãƒ­ãƒ¼ãƒ‰ã—æ•´å½¢ã™ã‚‹ã€€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã§ãã‚‹ã€‚
# ã“ã®æ®µéšã§ã¯ç”»åƒã®ãƒ‘ã‚¹ã®ãƒ†ãƒ³ã‚½ãƒ«
path_ds = tf.data.Dataset.from_tensor_slices(
            all_image_paths)

# ğŸŒŸã“ã“ã§ãƒ‘ã‚¹ã‚’ç”»åƒã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã™ã‚‹ãƒãƒƒãƒ—ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã€
#  ç”»åƒã®ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãªã‚‹ã€‚
image_ds = path_ds.map(
    load_and_preprocess_image, # ğŸŒŸã€€ã“ã“ã«decodeã™ã‚‹é–¢æ•°ã‚’è¨­å®šã™ã‚‹ã€‚
    num_parallel_calls=AUTOTUNE # ğŸŒŸ ã“ã“
    )

```

### <a name="Callback">ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯</a>

- [ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢é€£ãƒªãƒ³ã‚¯](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback)

- EarlyStop ã®ä»–ã«ã‚‚å­¦ç¿’ç‡ã€checkpoint ã®ä¿å­˜ã€tensorboard ã¸ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆãªã©ãŒã§ãã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚‚ç”¨æ„ã•ã‚Œã¦ã‚‹ã€‚

```python

# ğŸŒŸ ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½œã‚‹éš›ã¯ã“ã‚“ãªæ„Ÿã˜ã§Callbackã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿ã—ã¦ä½œæˆã™ã‚‹ã€‚
class PrintDot(keras.callbacks.Callback):
  # ã‚¨ãƒãƒƒã‚¯ãŒçµ‚ã‚ã‚‹ã”ã¨ã«ãƒ‰ãƒƒãƒˆã‚’ä¸€ã¤å‡ºåŠ›ã™ã‚‹ã“ã¨ã§é€²æ—ã‚’è¡¨ç¤º
  def on_epoch_end(self, epoch, logs):
    if epoch % 100 == 0: print('')
    print('.', end='')


# ğŸŒŸğŸŒŸ ãƒ¡ãƒˆãƒªãƒƒã‚¯ã«æ”¹å–„ãŒè¦‹ã‚‰ã‚Œãªã„å ´åˆã«çµ‚äº†ã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
# patience ã¯æ”¹å–„ãŒè¦‹ã‚‰ã‚Œã‚‹ã‹ã‚’ç›£è¦–ã™ã‚‹ã‚¨ãƒãƒƒã‚¯æ•°ã‚’è¡¨ã™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼
early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)

history = model.fit(
    normed_train_data,
    train_labels,
    epochs=EPOCHS,
    validation_split = 0.2,
    verbose=0,
    callbacks=[early_stop, PrintDot()])

```

---

## <a name="OverFitting">éå­¦ç¿’</a>

### éå­¦ç¿’ã®å¯¾ç­–ã«ä»¥ä¸‹ãŒæœ‰åŠ¹ã¨è¨€ã‚ã‚Œã¦ã„ã‚‹ã€‚

- [x] [è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™(**ãƒ‡ãƒ¼ã‚¿ã®æ”¹å–„**)](#DataExpand)
- [x] [data æ‹¡å¼µã‚’å®Ÿè¡Œ(**ãƒ‡ãƒ¼ã‚¿ã®æ”¹å–„**)](#DataExpand)
- [x] [dropout å±¤è¿½åŠ (**ãƒ¢ãƒ‡ãƒ«å´ã®æ”¹å–„ç­–**)](#dropOut)
- [x] [network ã®å®¹é‡ã‚’æ¸›ã‚‰ã™(**ãƒ¢ãƒ‡ãƒ«å´ã®æ”¹å–„ç­–**)](#SmallModel)
- [x] [**é‡ã¿ã®æ­£å‰‡åŒ–**(L1 æ­£å‰‡åŒ–ã€L2 æ­£å‰‡åŒ–ãªã©ã‚’ã—ã¦éå­¦ç¿’ã‚’é˜²ã)](#norm)
- [ ] ãƒãƒƒãƒæ­£è¦åŒ–

---

- <a name="DataExpand">è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™& data æ‹¡å¼µã‚’å®Ÿè¡Œ(**ãƒ‡ãƒ¼ã‚¿ã®æ”¹å–„**)</a>

  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ã€data æ‹¡å¼µã‚’å®Ÿè¡Œã¯ã»ã¼åŒç¾©ãªã®ã§ã€ã¾ã¨ã‚ã¦è¨˜è¼‰ã™ã‚‹ã€‚

    - **ã€Œè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ã€** ãã®ã¾ã¾ã®æ„å‘³

  - **data æ‹¡å¼µã‚’å®Ÿè¡Œ** ã¯ä¸‹è¨˜ã«ç¤ºã™ã€‚

- <a name="dropOut">dropout å±¤è¿½åŠ (**ãƒ¢ãƒ‡ãƒ«å´ã®æ”¹å–„ç­–**)</a>

  - **è¨“ç·´æ™‚**ã«å±¤ã‹ã‚‰å‡ºåŠ›ã•ã‚ŒãŸç‰¹å¾´é‡ã«å¯¾ã—ã¦ãƒ©ãƒ³ãƒ€ãƒ ã«ã€Œãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆï¼ˆã¤ã¾ã‚Šã‚¼ãƒ­åŒ–ï¼‰ã€ã‚’è¡Œã†ã‚‚ã®`ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡`ã¯æ¦‚ã­ `0.2 ~ 0.5` ãŒç›®å®‰ã¨è¨€ã‚ã‚Œã¦ã„ã‚‹ã€‚

  ```python
  # ğŸŒŸ ä½¿ã„æ–¹ ã“ã‚Œã‚’ãƒ¢ãƒ‡ãƒ«ã®å±¤ã«è¿½åŠ ã—ã¦ã„ãã ã‘ã€‚
  keras.layers.Dropout(0.5),
  ```

- <a name="SmallModel">network ã®å®¹é‡ã‚’æ¸›ã‚‰ã™(**ãƒ¢ãƒ‡ãƒ«å´ã®æ”¹å–„ç­–**)</a>

  - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¤šã„ã¨ä½™è¨ˆãªæ¡ä»¶ã«å¼•ã£ã‹ã‹ã‚‹ã‚ˆã†ã«ãªã‚Šã€å¢—ãˆã™ãã‚‹ã“ã¨ã§æ±åŒ–æ€§èƒ½ãŒä¸‹ãŒã£ã¦ã„ã(test ãƒ‡ãƒ¼ã‚¿ã§ã¯é«˜ã„æ­£è§£ç‡ã«ã‚‚é–¢ã‚ã‚‰ãšã€validation check ã«ã¦ä½ã„ã‚¹ã‚³ã‚¢ãŒå‡ºã¦ã—ã¾ã†ã€‚(**éå­¦ç¿’**))
  - ã‚»ã‚ªãƒªãƒ¼ã¨ã—ã¦å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã€å¤§ãã„ãƒ¢ãƒ‡ãƒ«ã®ä¸åº¦ã„ã„ã¨ã“ã‚ã‚’æ¢ã™å¿…è¦ãŒã‚ã‚‹ã€‚

- ä»¥ä¸‹ã«ä¾‹ã‚’ç¤ºã™ã€‚

```python
# -------------------------------------------- #
# ğŸŒŸã€€æ™®é€šã®ãƒ¢ãƒ‡ãƒ«
baseline_model = keras.Sequential([
    # `.summary` ã‚’è¦‹ã‚‹ãŸã‚ã«`input_shape`ãŒå¿…è¦
    keras.layers.Dense(16, activation='relu', input_shape=(NUM_WORDS,)),
    keras.layers.Dense(16, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

baseline_model.compile(optimizer='adam',
                       loss='binary_crossentropy',
                       metrics=['accuracy', 'binary_crossentropy'])

baseline_model.summary()
baseline_history = baseline_model.fit(train_data,
                                      train_labels,
                                      epochs=20,
                                      batch_size=512,
                                      validation_data=(test_data, test_labels),
                                      verbose=2)

# -------------------------------------------- #
# ğŸŒŸã€€å°ã•ã„ãƒ¢ãƒ‡ãƒ«
smaller_model = keras.Sequential([
    keras.layers.Dense(4, activation='relu', input_shape=(NUM_WORDS,)),
    keras.layers.Dense(4, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

smaller_model.compile(optimizer='adam',
                      loss='binary_crossentropy',
                      metrics=['accuracy', 'binary_crossentropy'])

smaller_model.summary()

smaller_history = smaller_model.fit(train_data,
                                    train_labels,
                                    epochs=20,
                                    batch_size=512,
                                    validation_data=(test_data, test_labels),
                                    verbose=2)
# -------------------------------------------- #
# ğŸŒŸã€€å¤§ãã„ãƒ¢ãƒ‡ãƒ«
bigger_model = keras.models.Sequential([
    keras.layers.Dense(512, activation='relu', input_shape=(NUM_WORDS,)),
    keras.layers.Dense(512, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])

bigger_model.compile(optimizer='adam',
                     loss='binary_crossentropy',
                     metrics=['accuracy','binary_crossentropy'])

bigger_model.summary()
bigger_history = bigger_model.fit(train_data, train_labels,
                                  epochs=20,
                                  batch_size=512,
                                  validation_data=(test_data, test_labels),
                                  verbose=2)
# -------------------------------------------- #


# ğŸŒŸ å¤§ä¸­å°ã®ãƒ¢ãƒ‡ãƒ«ã®binary_crossentropy,epochã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°
def plot_history(histories, key='binary_crossentropy'):
    plt.figure(figsize=(16,10))

    for name, history in histories:
        val = plt.plot(history.epoch, history.history['val_'+key],
                        '--', label=name.title()+' Val')
        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),
             label=name.title()+' Train')

    plt.xlabel('Epochs')
    plt.ylabel(key.replace('_',' ').title())
    plt.legend()

    plt.xlim([0,max(history.epoch)])

# ğŸŒŸãƒ¢ãƒ‡ãƒ«ã®histryã‚’æ¸¡ã™
plot_history([('baseline', baseline_history),
              ('smaller', smaller_history),
              ('bigger', bigger_history)])

```

- å¤§ã€ä¸­ã€å°ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè¡Œçµæœ

  - Baseline ã¨ Bigger ã® train,val ãŒæ—©ã€…ã«ä¹–é›¢ã—ã¦éå­¦ç¿’ã‚’èµ·ã“ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã‚‹ã€‚
    ![OverFit.png](OverFit.png)

<a name=norm>é‡ã¿ã®æ­£å‰‡åŒ–ã‚’åŠ ãˆã‚‹</a>

## é‡ã¿ã®æ­£å‰‡åŒ–ã‚’åŠ ãˆã‚‹

- ã€Œå˜ç´”ãªãƒ¢ãƒ‡ãƒ«ã€ã¨ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ã®åˆ†å¸ƒã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒå°ã•ã„ã‚‚ã®ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ãŒå°‘ãªã„ã‚‚ã®ï¼‰ã§ã™ã€‚  
  ã—ãŸãŒã£ã¦ã€éå­¦ç¿’ã‚’ç·©å’Œã™ã‚‹ãŸã‚ã®ä¸€èˆ¬çš„ãªæ‰‹æ³•ã¯ã€é‡ã¿ãŒå°ã•ã„å€¤ã®ã¿ã‚’ã¨ã‚‹ã“ã¨ã§ã€é‡ã¿å€¤ã®åˆ†å¸ƒãŒã‚ˆã‚Šæ•´ç„¶ã¨ãªã‚‹ï¼ˆæ­£å‰‡ï¼‰æ§˜ã«åˆ¶ç´„ã‚’ä¸ãˆã‚‹ã‚‚ã®ã§ã™ã€‚
- ã“ã‚Œã‚’**ã€Œé‡ã¿ã®æ­£å‰‡åŒ–ã€**ã¨å‘¼ã°ã‚Œã€**ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æå¤±é–¢æ•°ã«ã€é‡ã¿ã®å¤§ãã•ã«é–¢é€£ã™ã‚‹ã‚³ã‚¹ãƒˆã‚’åŠ ãˆã‚‹**ã“ã¨ã§è¡Œã‚ã‚Œã¾ã™ã€‚ã“ã®ã‚³ã‚¹ãƒˆã«ã¯ 2 ã¤ã®ç¨®é¡ãŒã‚ã‚Šã¾ã™ã€‚

  1. L1 æ­£å‰‡åŒ– é‡ã¿ä¿‚æ•°ã®çµ¶å¯¾å€¤ã«æ¯”ä¾‹ã™ã‚‹ã‚³ã‚¹ãƒˆã‚’åŠ ãˆã‚‹ï¼ˆé‡ã¿ã®ã€ŒL1 ãƒãƒ«ãƒ ã€ã¨å‘¼ã°ã‚Œã‚‹ï¼‰ã€‚
  2. L2 æ­£å‰‡åŒ– é‡ã¿ä¿‚æ•°ã®äºŒä¹—ã«æ¯”ä¾‹ã™ã‚‹ã‚³ã‚¹ãƒˆã‚’åŠ ãˆã‚‹ï¼ˆé‡ã¿ä¿‚æ•°ã®äºŒä¹—ã€ŒL2 ãƒãƒ«ãƒ ã€ã¨å‘¼ã°ã‚Œã‚‹ï¼‰ã€‚L2 æ­£å‰‡åŒ–ã¯ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”¨èªã§ã¯é‡ã¿æ¸›è¡°ï¼ˆWeight Decayï¼‰ã¨å‘¼ã°ã‚Œã‚‹ã€‚å‘¼ã³æ–¹ãŒé•ã†ã®ã§æ··ä¹±ã—ãªã„ã‚ˆã†ã«ã€‚é‡ã¿æ¸›è¡°ã¯æ•°å­¦çš„ã«ã¯ L2 æ­£å‰‡åŒ–ã¨åŒç¾©ã§ã‚ã‚‹ã€‚

- L1 æ­£å‰‡åŒ–ã¯é‡ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’ 0 ã«ã™ã‚‹ã“ã¨ã§ãƒ¢ãƒ‡ãƒ«ã‚’ç–ã«ã™ã‚‹åŠ¹æœãŒã‚ã‚Šã¾ã™ã€‚
- L2 æ­£å‰‡åŒ–ã¯é‡ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’åŠ ãˆã¾ã™ãŒãƒ¢ãƒ‡ãƒ«ã‚’ç–ã«ã™ã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

  - L2 æ­£å‰‡åŒ–ã®ã»ã†ãŒä¸€èˆ¬çš„ã§ã‚ã‚‹ç†ç”±ã®ä¸€ã¤ã§ã™ã€‚

- l2(0.001)ã¨ã„ã†ã®ã¯ã€**å±¤ã®é‡ã¿è¡Œåˆ—ã®ä¿‚æ•°å…¨ã¦ã«å¯¾ã—ã¦ 0.001x é‡ã¿ä¿‚æ•°ã®å€¤^2 ã‚’ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æå¤±å€¤åˆè¨ˆã«åŠ ãˆã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚**

- **ã“ã®ãƒšãƒŠãƒ«ãƒ†ã‚£ã¯è¨“ç·´æ™‚ã®ã¿ã«åŠ ãˆã‚‰ã‚Œã‚‹** ãŸã‚ã€**ã“ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æå¤±å€¤ã¯ã€è¨“ç·´æ™‚ã«ã¯ãƒ†ã‚¹ãƒˆæ™‚ã«æ¯”ã¹ã¦å¤§ãããªã‚‹**ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„

```python
l2_model = keras.models.Sequential([
    keras.layers.Dense(16,
        # ğŸŒŸ ãƒ‘ãƒ©ãƒ¡ã‚¿ã« l2ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§æ­£è¦åŒ–ã‚’è¡Œã†
        kernel_regularizer=keras.regularizers.l2(0.001),
        activation='relu', input_shape=(NUM_WORDS,)),
    keras.layers.Dense(16,
        kernel_regularizer=keras.regularizers.l2(0.001),
        activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')
])
```
