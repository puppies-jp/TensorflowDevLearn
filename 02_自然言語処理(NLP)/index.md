# [02\_è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰](<02_è‡ªç„¶è¨€èªå‡¦ç†(NLP)>)

- [ ] TensorFlow ã‚’ä½¿ã£ãŸè‡ªç„¶è¨€èªå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
- [ ] TensorFlow ã§ä½¿ç”¨ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”¨æ„ã™ã‚‹ã€‚
- [ ] äºŒé …åˆ†é¡ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ã‚«ãƒ†ã‚´ãƒªã‚’ç‰¹å®šã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹
- [ ] å¤šé …åˆ†é¡ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ã‚«ãƒ†ã‚´ãƒªã‚’ç‰¹å®šã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹
- [x] TensorFlow ãƒ¢ãƒ‡ãƒ«ã§å˜èªåŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
- [ ] äºŒé …åˆ†é¡ã¾ãŸã¯å¤šé …åˆ†é¡ã®ã„ãšã‚Œã‹ã®ãƒ¢ãƒ‡ãƒ«ã§ã€LSTM ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†é¡ã™ã‚‹ã€‚
- [ ] ãƒ¢ãƒ‡ãƒ«ã« RNN å±¤ã¨ GRU å±¤ã‚’è¿½åŠ ã™ã‚‹ã€‚
- [ ] ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã€RNNã€GRUã€CNN ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
- [ ] LSTM ã‚’æ—¢å­˜ã®ãƒ†ã‚­ã‚¹ãƒˆã§è¨“ç·´ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹(æ­Œã‚„è©©ãªã©)

---

## æ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆæ–¹æ³•

- ä»¥ä¸‹ã«æ‰‹é †ã‚’ã¾ã¨ã‚ã‚‹

    1. æ–‡å­—åˆ—ã‚’æ•´æ•°ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸé…åˆ—ã«ã—ã€å˜èªã®ãƒãƒƒãƒ—ã‚’ä½œæˆã™ã‚‹

    1. é…åˆ—ã‚’ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆï¼ˆone-hotï¼‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ã€‚

    1. é…åˆ—ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã«ã‚ˆã£ã¦åŒã˜é•·ã•ã«æƒãˆã€`(ã‚µãƒ³ãƒ—ãƒ«æ•° * è¨±å®¹ã§ãã‚‹é•·ã•ã®æœ€å¤§å€¤)` ã®å½¢ã®æ•´æ•°ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›´ã™ã‚‹ã€‚

        1. æ•°å€¤ã®é…åˆ—ã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«paddingã™ã‚‹æ–¹æ³•

            ```python
            train_data = 
                keras.preprocessing.sequence.pad_sequences(
                    train_data,
                    value=word_index["<PAD>"],
                    padding='post',
                    maxlen=256
                    )
                    
            test_data = 
                keras.preprocessing.sequence.pad_sequences(
                    test_data,
                    value=word_index["<PAD>"],
                    padding='post',
                    maxlen=256
                    )
            ```

        2. ğŸŒŸ(ã“ã“è¦šãˆã¨ãã“ã¨ï¼ï¼)textã‹ã‚‰å¤‰æ›ã™ã‚‹æ–¹æ³• 

            ```python
            def custom_standardization(input_data):
                lowercase = tf.strings.lower(input_data)
                
                # ğŸŒŸ <br>ã‚¿ã‚°ã‚’ã‘ã™ã€‚
                stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')

                # ğŸŒŸ punctuation == æ–‡å­—åˆ—ã®å¥èª­ç‚¹
                # ğŸŒŸ re.escape(pattern)
                #   pattern ä¸­ã®ç‰¹æ®Šæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¾ã™ã€‚
                #   ã“ã‚Œã¯æ­£è¦è¡¨ç¾ãƒ¡ã‚¿æ–‡å­—ã‚’å«ã¿ã†ã‚‹ä»»æ„ã®ãƒªãƒ†ãƒ©ãƒ«æ–‡å­—åˆ—ã«ãƒãƒƒãƒã—ãŸã„æ™‚ã«ä¾¿åˆ©ã§ã™ã€‚     
                return tf.strings.regex_replace(
                    stripped_html,
                    '[%s]' % re.escape(string.punctuation),
                    ''
                    )


            max_features = 10000
            sequence_length = 250
            vectorize_layer = layers.TextVectorization(
                standardize=custom_standardization,
                max_tokens=max_features,
                output_mode='int',
                output_sequence_length=sequence_length
                )
            
            # Make a text-only dataset (without labels), then call adapt
            train_text = raw_train_ds.map(lambda x, y: x)
            vectorize_layer.adapt(train_text)

            # ğŸŒŸ textã‚’vectoråŒ–ã—ã¦labelã¨ä¸€ç·’ã«å‡ºåŠ›ã™ã‚‹ã€‚
            def vectorize_text(text, label):
                text = tf.expand_dims(text, -1)
                return vectorize_layer(text), label

            # ğŸŒŸ ä½¿ç”¨ä¾‹1 :textã¨labelã‹ã‚‰Vectorã¨labelã‚’ç”Ÿæˆã™ã‚‹ã€‚
            # retrieve a batch (of 32 reviews and labels) from the dataset
            text_batch, label_batch = next(iter(raw_train_ds))
            first_review, first_label = text_batch[0], label_batch[0]
            print("Review", first_review)
            print("Label", raw_train_ds.class_names[first_label])
            print("Vectorized review", vectorize_text(first_review, first_label))

            # ğŸŒŸ datasetã‚’ä½œæˆã™ã‚‹ã€‚
            # ğŸŒŸ tf.keras.utils.text_dataset_from_directoryã«ã¦ä½œæˆã—ãŸdataset
            train_ds = raw_train_ds.map(vectorize_text)
            val_ds = raw_val_ds.map(vectorize_text)
            test_ds = raw_test_ds.map(vectorize_text)

            ```

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆæ–¹æ³•2

- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹æˆã‚’èª­ã¿è¾¼ã‚€
  - ã“ã‚“ãªæ„Ÿã˜ã®ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã§å„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ†ã‚­ã‚¹ãƒˆãŒå…¥ã£ã¦ã„ã‚‹æ§‹æˆã‚’å‰æã«è€ƒãˆã‚‹

```sh
user@MacBook text % tree -d aclImdb
aclImdb
â”œâ”€â”€ test # æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
â”‚Â Â  â”œâ”€â”€ neg # label1
â”‚Â Â  â””â”€â”€ pos # label2
â””â”€â”€ train  # å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    â”œâ”€â”€ neg
    â””â”€â”€ pos

7 directories
```

1. `text_dataset_from_directory`ã‚’ä½¿ã†
ã“ã†ã™ã‚‹ã“ã¨ã§trainã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ãã‚Œã‚‹ã€‚

```python
batch_size = 32
seed = 42

raw_train_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='training', 
    seed=seed)
```