# [02\_è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰](<02_è‡ªç„¶è¨€èªå‡¦ç†(NLP)>)

- [ ] TensorFlow ã‚’ä½¿ã£ãŸè‡ªç„¶è¨€èªå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
- [x] TensorFlow ã§ä½¿ç”¨ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”¨æ„ã™ã‚‹ã€‚
- [x] äºŒé …åˆ†é¡ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ã‚«ãƒ†ã‚´ãƒªã‚’ç‰¹å®šã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹
- [ ] å¤šé …åˆ†é¡ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ã‚«ãƒ†ã‚´ãƒªã‚’ç‰¹å®šã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹
- [x] TensorFlow ãƒ¢ãƒ‡ãƒ«ã§å˜èªåŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
- [ ] äºŒé …åˆ†é¡ã¾ãŸã¯å¤šé …åˆ†é¡ã®ã„ãšã‚Œã‹ã®ãƒ¢ãƒ‡ãƒ«ã§ã€LSTM ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†é¡ã™ã‚‹ã€‚
- [ ] ãƒ¢ãƒ‡ãƒ«ã« RNN å±¤ã¨ GRU å±¤ã‚’è¿½åŠ ã™ã‚‹ã€‚
- [ ] ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã€RNNã€GRUã€CNN ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
- [ ] LSTM ã‚’æ—¢å­˜ã®ãƒ†ã‚­ã‚¹ãƒˆã§è¨“ç·´ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹(æ­Œã‚„è©©ãªã©)

---

ãƒªãƒ³ã‚¯

- [ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ](#dataset)
- [ãƒ•ã‚©ãƒ«ãƒ€æ§‹æˆã”ã¨ã«ã¾ã¨ã¾ã£ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ](#DirDataset)
- [ãƒ¢ãƒ‡ãƒ«ä½œæˆ(åŸºæœ¬çš„)](#basic_model)
- [RNNé©ç”¨ãƒ¢ãƒ‡ãƒ«](#rnn_model)
- [BERTãƒ¢ãƒ‡ãƒ«](#BERT_model)

---

## <a name=dataset>æ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆæ–¹æ³•</a>

- ä»¥ä¸‹ã«æ‰‹é †ã‚’ã¾ã¨ã‚ã‚‹

    1. æ–‡å­—åˆ—ã‚’æ•´æ•°ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸé…åˆ—ã«ã—ã€å˜èªã®ãƒãƒƒãƒ—ã‚’ä½œæˆã™ã‚‹

    1. é…åˆ—ã‚’ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆï¼ˆone-hotï¼‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ã€‚

    1. é…åˆ—ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã«ã‚ˆã£ã¦åŒã˜é•·ã•ã«æƒãˆã€`(ã‚µãƒ³ãƒ—ãƒ«æ•° * è¨±å®¹ã§ãã‚‹é•·ã•ã®æœ€å¤§å€¤)` ã®å½¢ã®æ•´æ•°ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›´ã™ã‚‹ã€‚

        1. æ•°å€¤ã®é…åˆ—ã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«paddingã™ã‚‹æ–¹æ³•

            ```python
            train_data = 
                keras.preprocessing.sequence.pad_sequences(
                    train_data,
                    value=word_index["<PAD>"],
                    padding='post',
                    maxlen=256
                    )
                    
            test_data = 
                keras.preprocessing.sequence.pad_sequences(
                    test_data,
                    value=word_index["<PAD>"],
                    padding='post',
                    maxlen=256
                    )
            ```

        2. ğŸŒŸ(ã“ã“è¦šãˆã¨ãã“ã¨ï¼ï¼)textã‹ã‚‰å¤‰æ›ã™ã‚‹æ–¹æ³• 

            ```python
            def custom_standardization(input_data):
                lowercase = tf.strings.lower(input_data)
                
                # ğŸŒŸ <br>ã‚¿ã‚°ã‚’ã‘ã™ã€‚
                stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')

                # ğŸŒŸ punctuation == æ–‡å­—åˆ—ã®å¥èª­ç‚¹
                # ğŸŒŸ re.escape(pattern)
                #   pattern ä¸­ã®ç‰¹æ®Šæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¾ã™ã€‚
                #   ã“ã‚Œã¯æ­£è¦è¡¨ç¾ãƒ¡ã‚¿æ–‡å­—ã‚’å«ã¿ã†ã‚‹ä»»æ„ã®ãƒªãƒ†ãƒ©ãƒ«æ–‡å­—åˆ—ã«ãƒãƒƒãƒã—ãŸã„æ™‚ã«ä¾¿åˆ©ã§ã™ã€‚     
                return tf.strings.regex_replace(
                    stripped_html,
                    '[%s]' % re.escape(string.punctuation),
                    ''
                    )


            max_features = 10000
            sequence_length = 250
            vectorize_layer = layers.TextVectorization(
                standardize=custom_standardization,
                max_tokens=max_features,
                output_mode='int',
                output_sequence_length=sequence_length
                )
            
            # Make a text-only dataset (without labels), then call adapt
            train_text = raw_train_ds.map(lambda x, y: x)
            vectorize_layer.adapt(train_text)

            # ğŸŒŸ textã‚’vectoråŒ–ã—ã¦labelã¨ä¸€ç·’ã«å‡ºåŠ›ã™ã‚‹ã€‚
            def vectorize_text(text, label):
                text = tf.expand_dims(text, -1)
                return vectorize_layer(text), label

            # ğŸŒŸ ä½¿ç”¨ä¾‹1 :textã¨labelã‹ã‚‰Vectorã¨labelã‚’ç”Ÿæˆã™ã‚‹ã€‚
            # retrieve a batch (of 32 reviews and labels) from the dataset
            text_batch, label_batch = next(iter(raw_train_ds))
            first_review, first_label = text_batch[0], label_batch[0]
            print("Review", first_review)
            print("Label", raw_train_ds.class_names[first_label])
            print("Vectorized review", vectorize_text(first_review, first_label))

            # ğŸŒŸ datasetã‚’ä½œæˆã™ã‚‹ã€‚
            # ğŸŒŸ tf.keras.utils.text_dataset_from_directoryã«ã¦ä½œæˆã—ãŸdataset
            train_ds = raw_train_ds.map(vectorize_text)
            val_ds = raw_val_ds.map(vectorize_text)
            test_ds = raw_test_ds.map(vectorize_text)

            ```

## <a name=DirDataset>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆæ–¹æ³•2</a>

- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹æˆã‚’èª­ã¿è¾¼ã‚€
  - ã“ã‚“ãªæ„Ÿã˜ã®ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã§å„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ†ã‚­ã‚¹ãƒˆãŒå…¥ã£ã¦ã„ã‚‹æ§‹æˆã‚’å‰æã«è€ƒãˆã‚‹

```sh
user@MacBook text % tree -d aclImdb
aclImdb
â”œâ”€â”€ test # æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
â”‚Â Â  â”œâ”€â”€ neg # label1
â”‚Â Â  â””â”€â”€ pos # label2
â””â”€â”€ train  # å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    â”œâ”€â”€ neg
    â””â”€â”€ pos

7 directories
```

1. `text_dataset_from_directory`ã‚’ä½¿ã†
ã“ã†ã™ã‚‹ã“ã¨ã§trainã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ãã‚Œã‚‹ã€‚

```python
batch_size = 32
seed = 42

# ğŸŒŸtrainãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
raw_train_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='training', 
    seed=seed)

# ğŸŒŸvalidation ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
raw_val_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='validation', 
    seed=seed)

for index in range(len(raw_train_ds.class_names)):
    # ğŸŒŸ indexã¨ãƒ©ãƒ™ãƒ«ãŒå¯¾å¿œã¥ã‘ã‚‰ã‚Œã¦ã‚‹ã€‚
    print(f"Label {index} corresponds to {raw_train_ds.class_names[index]}")
    
def custom_standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')
  return tf.strings.regex_replace(stripped_html,
                                  '[%s]' % re.escape(string.punctuation),
                                  '')

# ğŸŒŸã“ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å…±é€šåŒ–ã—ãªã„ã¨ã‚ã‘ãŒã‚ã‹ã‚‰ãªããªã‚‹
max_features = 10000ã€€# ğŸŒŸå˜èªæ•°
sequence_length = 250 # ğŸŒŸdatasetã®æœ€å¤§æ–‡å­—åˆ—é•·

vectorize_layer = layers.TextVectorization(
    standardize=custom_standardization,
    max_tokens=max_features,ã€€# ğŸŒŸå˜èªæ•°
    output_mode='int',
    # ğŸŒŸdatasetã®æœ€å¤§æ–‡å­—åˆ—é•·(æŒ‡å®šã—ãªã„ã¨æœ€å¤§æ–‡å­—åˆ—é•·ã«åˆã‚ã›ã‚‰ã‚Œã‚‹)
    output_sequence_length=sequence_lengthã€€
    )

def vectorize_text(text, label):
    text = tf.expand_dims(text, -1)
    return vectorize_layer(text), label 

train_ds = raw_train_ds.map(vectorize_text)
val_ds = raw_val_ds.map(vectorize_text)

# ğŸŒŸcashe,prefchã‚’è¨­å®šã™ã‚‹ã€‚
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
```

## <a name=basic_model>ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ</a>

```python
embedding_dim = 16

model = tf.keras.Sequential([
  layers.Embedding(max_features + 1, embedding_dim),
  layers.Dropout(0.2),
  layers.GlobalAveragePooling1D(),
  layers.Dropout(0.2),
  layers.Dense(1)])

model.summary()
```

```python
# ğŸŒŸğŸŒŸmodelã®ä½œã‚Šæ–¹2ğŸŒŸğŸŒŸ
export_model = tf.keras.Sequential([
  ## ğŸŒŸã“ã“ã«vectorizeå±¤ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ç›´æ¥çš„ã«æ–‡å­—åˆ—ã‚’å…¥åŠ›ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚
  vectorize_layer, 
  model,
  layers.Activation('sigmoid')
])

export_model.compile(
    loss=losses.BinaryCrossentropy(from_logits=False), 
    optimizer="adam", 
    metrics=['accuracy']
)

# Test it with `raw_test_ds`, which yields raw strings
loss, accuracy = export_model.evaluate(raw_test_ds)
print(accuracy)

examples = [
  "The movie was great!",
  "The movie was okay.",
  "The movie was terrible..."
]

export_model.predict(examples)
```

## <a name=rnn_model>RNNã‚’é©ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«</a>

- RNNå±¤1ã¤ã ã‘ã®ãƒ¢ãƒ‡ãƒ«

```python
model = tf.keras.Sequential([
    encoder,
    tf.keras.layers.Embedding(
        input_dim=len(encoder.get_vocabulary()),
        output_dim=64,
        # Use masking to handle the variable sequence lengths
        mask_zero=True),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
])
```

- RNNã‚’2å±¤ã‚¹ã‚¿ãƒƒã‚¯ã—ãŸãƒ¢ãƒ‡ãƒ«
  - ğŸŒŸ`return_sequences`ã®ä½¿ã„æ–¹ã«æ³¨ç›®
    - true: æ¯å›å…¥åŠ›ã”ã¨ã«returnã™ã‚‹
    - false: æœ€å¾Œã«å‡ºåŠ›ã™ã‚‹ã€‚

```python
model = tf.keras.Sequential([
    encoder,
    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),
    # ğŸŒŸreturn_sequences=trueã¨ã™ã‚‹ã“ã¨ã§æ¯å›returnã™ã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹ã€‚
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1)
])
```

## <a name=BERT_model>BERTã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«</a>

- ãã‚‚ãã‚‚BERTã¨ã¯ã€ã€ã€

  > BERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). 
  > They compute vector-space representations of natural language that are suitable for use in deep learning models. 
  > The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers.

  - ã¤ã¾ã‚Šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¿ãŸã„ãªã‚‚ã®ï¼Ÿ

  - BERTãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
    - tfhubã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã™ã€‚

        <details><summary>(â–¶ï¸click it)BERTãƒ¢ãƒ‡ãƒ«ç¾¤(ã“ã‚Œã‚‰ã‹ã‚‰é‡ã¿ã‚’é¸ã¹ã‚‹)</summary><div>

        <pre><code>
        bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' 

        map_name_to_handle = {
            'bert_en_uncased_L-12_H-768_A-12':
                'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',
            'bert_en_cased_L-12_H-768_A-12':
                'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',
            'bert_multi_cased_L-12_H-768_A-12':
                'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',
            'small_bert/bert_en_uncased_L-2_H-128_A-2':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',
            'small_bert/bert_en_uncased_L-2_H-256_A-4':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',
            'small_bert/bert_en_uncased_L-2_H-512_A-8':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',
            'small_bert/bert_en_uncased_L-2_H-768_A-12':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',
            'small_bert/bert_en_uncased_L-4_H-128_A-2':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',
            'small_bert/bert_en_uncased_L-4_H-256_A-4':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',
            'small_bert/bert_en_uncased_L-4_H-512_A-8':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',
            'small_bert/bert_en_uncased_L-4_H-768_A-12':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',
            'small_bert/bert_en_uncased_L-6_H-128_A-2':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',
            'small_bert/bert_en_uncased_L-6_H-256_A-4':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',
            'small_bert/bert_en_uncased_L-6_H-512_A-8':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',
            'small_bert/bert_en_uncased_L-6_H-768_A-12':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',
            'small_bert/bert_en_uncased_L-8_H-128_A-2':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',
            'small_bert/bert_en_uncased_L-8_H-256_A-4':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',
            'small_bert/bert_en_uncased_L-8_H-512_A-8':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',
            'small_bert/bert_en_uncased_L-8_H-768_A-12':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',
            'small_bert/bert_en_uncased_L-10_H-128_A-2':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',
            'small_bert/bert_en_uncased_L-10_H-256_A-4':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',
            'small_bert/bert_en_uncased_L-10_H-512_A-8':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',
            'small_bert/bert_en_uncased_L-10_H-768_A-12':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',
            'small_bert/bert_en_uncased_L-12_H-128_A-2':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',
            'small_bert/bert_en_uncased_L-12_H-256_A-4':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',
            'small_bert/bert_en_uncased_L-12_H-512_A-8':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',
            'small_bert/bert_en_uncased_L-12_H-768_A-12':
                'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',
            'albert_en_base':
                'https://tfhub.dev/tensorflow/albert_en_base/2',
            'electra_small':
                'https://tfhub.dev/google/electra_small/2',
            'electra_base':
                'https://tfhub.dev/google/electra_base/2',
            'experts_pubmed':
                'https://tfhub.dev/google/experts/bert/pubmed/2',
            'experts_wiki_books':
                'https://tfhub.dev/google/experts/bert/wiki_books/2',
            'talking-heads_base':
                'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',
        }

        map_model_to_preprocess = {
            'bert_en_uncased_L-12_H-768_A-12':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'bert_en_cased_L-12_H-768_A-12':
                'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',
            'small_bert/bert_en_uncased_L-2_H-128_A-2':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-2_H-256_A-4':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-2_H-512_A-8':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-2_H-768_A-12':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-4_H-128_A-2':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-4_H-256_A-4':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-4_H-512_A-8':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-4_H-768_A-12':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-6_H-128_A-2':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-6_H-256_A-4':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-6_H-512_A-8':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-6_H-768_A-12':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-8_H-128_A-2':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-8_H-256_A-4':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-8_H-512_A-8':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-8_H-768_A-12':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-10_H-128_A-2':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-10_H-256_A-4':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-10_H-512_A-8':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-10_H-768_A-12':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-12_H-128_A-2':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-12_H-256_A-4':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-12_H-512_A-8':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'small_bert/bert_en_uncased_L-12_H-768_A-12':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'bert_multi_cased_L-12_H-768_A-12':
                'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',
            'albert_en_base':
                'https://tfhub.dev/tensorflow/albert_en_preprocess/3',
            'electra_small':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'electra_base':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'experts_pubmed':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'experts_wiki_books':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
            'talking-heads_base':
                'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
        }

        tfhub_handle_encoder = map_name_to_handle[bert_model_name]
        tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]

        print(f'BERT model selected           : {tfhub_handle_encoder}')
        print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')
        </code></pre>

        </div></details>

    - ğŸŒŸåŸºæœ¬ã“ã‚Œã‚’é¸ã‚“ã©ã‘ã°OK

        ```python
        tfhub_handle_encoder = "https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1"
        tfhub_handle_preprocess = "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3"

        # ğŸŒŸğŸŒŸğŸŒŸã€€ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã€€ğŸŒŸğŸŒŸğŸŒŸ 
        ## ğŸŒŸãƒ—ãƒªãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«(ã‚ãã¾ã§preprocessãƒ¢ãƒ‡ãƒ«)
        bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)

        # ğŸŒŸã–ã£ãã‚Šã—ãŸä½¿ã„æ–¹ 
        text_test = ['this is such an amazing movie!']
        text_preprocessed = bert_preprocess_model(text_test)

        print(f'Keys       : {list(text_preprocessed.keys())}')
        print(f'Shape      : {text_preprocessed["input_word_ids"].shape}')
        print(f'Word Ids   : {text_preprocessed["input_word_ids"][0, :12]}')
        print(f'Input Mask : {text_preprocessed["input_mask"][0, :12]}')
        print(f'Type Ids   : {text_preprocessed["input_type_ids"][0, :12]}')

        # ğŸŒŸğŸŒŸğŸŒŸBERTãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ğŸŒŸğŸŒŸğŸŒŸ
        ## ğŸŒŸ ã‚ãã¾ã§ã“ã“ã§èª­ã¿è¾¼ã‚“ã§ã„ã‚‹ã“ã¨ã«æ³¨æ„
        bert_model = hub.KerasLayer(tfhub_handle_encoder)
        
        # ğŸŒŸğŸŒŸä½¿ã£ã¦ã¿ã‚‹
        # ãƒ—ãƒªãƒ—ãƒ­ã‚»ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›çµæœã‚’æ¸¡ã—ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„
        bert_results = bert_model(text_preprocessed)

        print(f'Loaded BERT: {tfhub_handle_encoder}')
        print(f'Pooled Outputs Shape:{bert_results["pooled_output"].shape}')
        print(f'Pooled Outputs Values:{bert_results["pooled_output"][0, :12]}')
        print(f'Sequence Outputs Shape:{bert_results["sequence_output"].shape}')
        print(f'Sequence Outputs Values:{bert_results["sequence_output"][0, :12]}')
        ```

