# [02\_è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰](<02_è‡ªç„¶è¨€èªå‡¦ç†(NLP)>)

- [ ] TensorFlow ã‚’ä½¿ã£ãŸè‡ªç„¶è¨€èªå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
- [x] TensorFlow ã§ä½¿ç”¨ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”¨æ„ã™ã‚‹ã€‚
- [x] äºŒé …åˆ†é¡ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ã‚«ãƒ†ã‚´ãƒªã‚’ç‰¹å®šã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹
- [ ] å¤šé …åˆ†é¡ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ã‚«ãƒ†ã‚´ãƒªã‚’ç‰¹å®šã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹
- [x] TensorFlow ãƒ¢ãƒ‡ãƒ«ã§å˜èªåŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
- [ ] äºŒé …åˆ†é¡ã¾ãŸã¯å¤šé …åˆ†é¡ã®ã„ãšã‚Œã‹ã®ãƒ¢ãƒ‡ãƒ«ã§ã€LSTM ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†é¡ã™ã‚‹ã€‚
- [ ] ãƒ¢ãƒ‡ãƒ«ã« RNN å±¤ã¨ GRU å±¤ã‚’è¿½åŠ ã™ã‚‹ã€‚
- [ ] ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã€RNNã€GRUã€CNN ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
- [ ] LSTM ã‚’æ—¢å­˜ã®ãƒ†ã‚­ã‚¹ãƒˆã§è¨“ç·´ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹(æ­Œã‚„è©©ãªã©)

---

## æ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆæ–¹æ³•

- ä»¥ä¸‹ã«æ‰‹é †ã‚’ã¾ã¨ã‚ã‚‹

    1. æ–‡å­—åˆ—ã‚’æ•´æ•°ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸé…åˆ—ã«ã—ã€å˜èªã®ãƒãƒƒãƒ—ã‚’ä½œæˆã™ã‚‹

    1. é…åˆ—ã‚’ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆï¼ˆone-hotï¼‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ã€‚

    1. é…åˆ—ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã«ã‚ˆã£ã¦åŒã˜é•·ã•ã«æƒãˆã€`(ã‚µãƒ³ãƒ—ãƒ«æ•° * è¨±å®¹ã§ãã‚‹é•·ã•ã®æœ€å¤§å€¤)` ã®å½¢ã®æ•´æ•°ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›´ã™ã‚‹ã€‚

        1. æ•°å€¤ã®é…åˆ—ã‚’ã‚·ãƒ³ãƒ—ãƒ«ã«paddingã™ã‚‹æ–¹æ³•

            ```python
            train_data = 
                keras.preprocessing.sequence.pad_sequences(
                    train_data,
                    value=word_index["<PAD>"],
                    padding='post',
                    maxlen=256
                    )
                    
            test_data = 
                keras.preprocessing.sequence.pad_sequences(
                    test_data,
                    value=word_index["<PAD>"],
                    padding='post',
                    maxlen=256
                    )
            ```

        2. ğŸŒŸ(ã“ã“è¦šãˆã¨ãã“ã¨ï¼ï¼)textã‹ã‚‰å¤‰æ›ã™ã‚‹æ–¹æ³• 

            ```python
            def custom_standardization(input_data):
                lowercase = tf.strings.lower(input_data)
                
                # ğŸŒŸ <br>ã‚¿ã‚°ã‚’ã‘ã™ã€‚
                stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')

                # ğŸŒŸ punctuation == æ–‡å­—åˆ—ã®å¥èª­ç‚¹
                # ğŸŒŸ re.escape(pattern)
                #   pattern ä¸­ã®ç‰¹æ®Šæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã—ã¾ã™ã€‚
                #   ã“ã‚Œã¯æ­£è¦è¡¨ç¾ãƒ¡ã‚¿æ–‡å­—ã‚’å«ã¿ã†ã‚‹ä»»æ„ã®ãƒªãƒ†ãƒ©ãƒ«æ–‡å­—åˆ—ã«ãƒãƒƒãƒã—ãŸã„æ™‚ã«ä¾¿åˆ©ã§ã™ã€‚     
                return tf.strings.regex_replace(
                    stripped_html,
                    '[%s]' % re.escape(string.punctuation),
                    ''
                    )


            max_features = 10000
            sequence_length = 250
            vectorize_layer = layers.TextVectorization(
                standardize=custom_standardization,
                max_tokens=max_features,
                output_mode='int',
                output_sequence_length=sequence_length
                )
            
            # Make a text-only dataset (without labels), then call adapt
            train_text = raw_train_ds.map(lambda x, y: x)
            vectorize_layer.adapt(train_text)

            # ğŸŒŸ textã‚’vectoråŒ–ã—ã¦labelã¨ä¸€ç·’ã«å‡ºåŠ›ã™ã‚‹ã€‚
            def vectorize_text(text, label):
                text = tf.expand_dims(text, -1)
                return vectorize_layer(text), label

            # ğŸŒŸ ä½¿ç”¨ä¾‹1 :textã¨labelã‹ã‚‰Vectorã¨labelã‚’ç”Ÿæˆã™ã‚‹ã€‚
            # retrieve a batch (of 32 reviews and labels) from the dataset
            text_batch, label_batch = next(iter(raw_train_ds))
            first_review, first_label = text_batch[0], label_batch[0]
            print("Review", first_review)
            print("Label", raw_train_ds.class_names[first_label])
            print("Vectorized review", vectorize_text(first_review, first_label))

            # ğŸŒŸ datasetã‚’ä½œæˆã™ã‚‹ã€‚
            # ğŸŒŸ tf.keras.utils.text_dataset_from_directoryã«ã¦ä½œæˆã—ãŸdataset
            train_ds = raw_train_ds.map(vectorize_text)
            val_ds = raw_val_ds.map(vectorize_text)
            test_ds = raw_test_ds.map(vectorize_text)

            ```

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆæ–¹æ³•2

- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹æˆã‚’èª­ã¿è¾¼ã‚€
  - ã“ã‚“ãªæ„Ÿã˜ã®ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã§å„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ†ã‚­ã‚¹ãƒˆãŒå…¥ã£ã¦ã„ã‚‹æ§‹æˆã‚’å‰æã«è€ƒãˆã‚‹

```sh
user@MacBook text % tree -d aclImdb
aclImdb
â”œâ”€â”€ test # æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
â”‚Â Â  â”œâ”€â”€ neg # label1
â”‚Â Â  â””â”€â”€ pos # label2
â””â”€â”€ train  # å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    â”œâ”€â”€ neg
    â””â”€â”€ pos

7 directories
```

1. `text_dataset_from_directory`ã‚’ä½¿ã†
ã“ã†ã™ã‚‹ã“ã¨ã§trainã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ãã‚Œã‚‹ã€‚

```python
batch_size = 32
seed = 42

# ğŸŒŸtrainãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
raw_train_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='training', 
    seed=seed)

# ğŸŒŸvalidation ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
raw_val_ds = tf.keras.utils.text_dataset_from_directory(
    'aclImdb/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='validation', 
    seed=seed)

for index in range(len(raw_train_ds.class_names)):
    # ğŸŒŸ indexã¨ãƒ©ãƒ™ãƒ«ãŒå¯¾å¿œã¥ã‘ã‚‰ã‚Œã¦ã‚‹ã€‚
    print(f"Label {index} corresponds to {raw_train_ds.class_names[index]}")
    
def custom_standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')
  return tf.strings.regex_replace(stripped_html,
                                  '[%s]' % re.escape(string.punctuation),
                                  '')

# ğŸŒŸã“ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å…±é€šåŒ–ã—ãªã„ã¨ã‚ã‘ãŒã‚ã‹ã‚‰ãªããªã‚‹
max_features = 10000ã€€# ğŸŒŸå˜èªæ•°
sequence_length = 250 # ğŸŒŸdatasetã®æœ€å¤§æ–‡å­—åˆ—é•·

vectorize_layer = layers.TextVectorization(
    standardize=custom_standardization,
    max_tokens=max_features,ã€€# ğŸŒŸå˜èªæ•°
    output_mode='int',
    output_sequence_length=sequence_lengthã€€# ğŸŒŸdatasetã®æœ€å¤§æ–‡å­—åˆ—é•·
    )

def vectorize_text(text, label):
    text = tf.expand_dims(text, -1)
    return vectorize_layer(text), label 

train_ds = raw_train_ds.map(vectorize_text)
val_ds = raw_val_ds.map(vectorize_text)

# ğŸŒŸcashe,prefchã‚’è¨­å®šã™ã‚‹ã€‚
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
```

## ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ

```python
embedding_dim = 16

model = tf.keras.Sequential([
  layers.Embedding(max_features + 1, embedding_dim),
  layers.Dropout(0.2),
  layers.GlobalAveragePooling1D(),
  layers.Dropout(0.2),
  layers.Dense(1)])

model.summary()
```

```python
# ğŸŒŸğŸŒŸmodelã®ä½œã‚Šæ–¹2ğŸŒŸğŸŒŸ
export_model = tf.keras.Sequential([
  ## ğŸŒŸã“ã“ã«vectorizeå±¤ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ç›´æ¥çš„ã«æ–‡å­—åˆ—ã‚’å…¥åŠ›ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚
  vectorize_layer, 
  model,
  layers.Activation('sigmoid')
])

export_model.compile(
    loss=losses.BinaryCrossentropy(from_logits=False), 
    optimizer="adam", 
    metrics=['accuracy']
)

# Test it with `raw_test_ds`, which yields raw strings
loss, accuracy = export_model.evaluate(raw_test_ds)
print(accuracy)

examples = [
  "The movie was great!",
  "The movie was okay.",
  "The movie was terrible..."
]

export_model.predict(examples)

```